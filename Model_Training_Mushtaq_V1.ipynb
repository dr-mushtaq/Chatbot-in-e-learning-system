{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dr-mushtaq/Chatbot-in-e-learning-system/blob/main/Model_Training_Mushtaq_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**Table of Content**</p>"
      ],
      "metadata": {
        "id": "AfpLfvDY9h56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Import Library\n",
        "2.   Import Dataset\n",
        "3.   Data Preprocessing\n",
        "4.   Model Training\n",
        "\n"
      ],
      "metadata": {
        "id": "iXHrYhxO9reC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HLNoZfkBUGc"
      },
      "source": [
        "\n",
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**1-Import library**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UkuJaCW9bT6u"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbN0QbsScujH",
        "outputId": "0309819e-7ced-4cd6-a60b-cebc6f4e03ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpC21eawvwSY"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**2-Import Dataset**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R60NmwbGDNp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc215e45-2263-4605-b758-8f8839775cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "data_file = open('/content/gdrive/MyDrive/Research /Research Projects/Chatbot System in Education /Dataset/intents.json').read()\n",
        "intents = json.loads(data_file)"
      ],
      "metadata": {
        "id": "OKUopk7KNCdp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yl5GZnxZnI3"
      },
      "source": [
        "\n",
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**3-Data Preprocessing**</p>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "5TPXLd1qLTKX",
        "outputId": "e76591b3-e64d-4c16-d7dd-d25d6758f3fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ICA7KJTRD768"
      },
      "outputs": [],
      "source": [
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "\n",
        "        #tokenize each word\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "        #add documents in the corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import nltk\n",
        " nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXBIEfUANo8e",
        "outputId": "dab35e14-696e-41fc-d502-122eb4f3588b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-azsX0L8c7hX",
        "outputId": "04ae6b44-c57d-4b8d-ff09-1d5ac392aa2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "235 documents\n",
            "93 classes [' ADMISSION OF FOREIGN/DUAL NATIONAL', ' Additional Specialization Certificate', ' Admission Status', ' Alumni Association ', ' Citizen complaint portal ', ' Club and Societies ', ' Courses Catalogue', ' Degree/Transcript Verification ', ' Digiskills ', ' Disabled Student Fee structure(Local)', ' Disabled Student Fee structure(Overseas)', ' Exam Superintendent Registration ', ' FACULTY OF SCIENCE & TECHNOLOGY Programs', ' Faculty of Arts Programs', ' Faculty of Education Programs', ' Faculty of Management Programs', ' Grading Scheme', ' How to Apply Online', ' Life at VU', ' New Registration', ' ORIC ', ' PROFESSIONAL COURSES DEVELOPMENT', ' PROTECTION AGAINST HARASSMENT  ', ' Prospectus', ' RIGHT OF ACCESS TO INFORMATION ', ' Recent Advertisements of VU', ' Schedule of other Charges', ' Short Certificate Course', ' Student Startup', ' Study Scheme', ' Suitability of  VU', ' Tender', ' Vendor Registration ', ' Zero Semester', 'Academic Calendar', 'Acceptable abroad ', 'Admission through Course Exemption/Transfer of Credit Hour(S)', 'Admission through Entry Option', 'Campus changed', 'Credit', 'Dev C++', 'Educator Scholarships', 'Fee Structure(Local)', 'Fee Structure(Overseas)', 'HEC Recongnized ', 'Lecture Schedule', 'NBP Student Loan Scheme', 'PAKISTAN BAIT-UL-MAL STIPENDS', 'PUNJAB EDUCATIONAL ENDOWMENT FUND (PEEF) ', 'PUNJAB WORKERS WELFARE BOARD', 'Private Institution ', 'QEC ', 'Student Handbook', 'Syeda Mubarik Begum Scholarships', 'Usefull Links ', 'VSpace ', 'VU Bookshop', 'VU Professors ', 'VU Scholarships', 'about', 'add or dropcourses', 'admission', 'appear for exam', 'ask question from teacher', 'campus change', 'comparable courses', 'complaint', 'concern certificate', 'contact admission department', 'continue education after completing degree', 'course selection', 'course syllabus', 'create account', 'duplicateid', 'expensive education', 'freeze account', 'goodbye', 'greeting', 'help', 'how to study', 'institution', 'lecture handouts and dvd', 'migration certificate', 'name', 'operate computer', 'professors', 'recognized degree', 'standards of education', 'student status (Overseas to Pakistan and Vice Versa)', 'student status updation', 'thanks', 'uniform education', 'where to study']\n",
            "290 unique lemmatized words ['&', \"''\", \"'s\", '(', ')', '10', '16', '``', 'a', 'able', 'about', 'abroad', 'academic', 'acceptable', 'account', 'act', 'add', 'additional', 'admission', 'admitted', 'advertisement', 'after', 'against', 'all', 'alumnus', 'an', 'and', 'any', 'anyone', 'appear', 'apply', 'are', 'art', 'ask', 'association', 'at', 'b', 'bait', 'bait-ul-mal', 'be', 'begum', 'board', 'bookshop', 'by', 'bye', 'c++', 'calendar', 'calender', 'call', 'campus', 'can', 'candidate', 'card', 'catalogue', 'cell', 'certificate', 'certificate/no', 'certifucate', 'change', 'changed', 'charge', 'check', 'club', 'comparable', 'complaint', 'completing', 'computer', 'concern', 'contact', 'content', 'contents/syllabus', 'continue', 'could', 'country', 'course', 'create', 'credit', 'degree', 'department', 'dev', 'develop', 'development', 'different', 'digiskills', 'disabled', 'discipline', 'do', 'doe', 'download', 'drop', 'duplicate', 'dvd', 'dy', 'education', 'educator', 'elearning', 'endowment', 'enhancement', 'entry', 'exam', 'exemption', 'expensive', 'faculty', 'fee', 'for', 'foreign/dual', 'form', 'freeze', 'freeze/unfreeze', 'from', 'fund', 'general', 'get', 'give', 'goodbye', 'government', 'grading', 'guidlines', 'hand', 'handbook', 'handling', 'handout', 'harassment', 'have', 'hay', 'hec', 'hello', 'help', 'helpful', 'hey', 'hi', 'holding', 'hour', 'how', 'i', 'id', 'if', 'in', 'institution', 'international', 'is', 'it', 'k', 'karien', 'keise', 'ki', 'later', 'latest', 'learn', 'lectuer', 'lecture', 'life', 'link', 'list', 'liye', 'loan', 'local', 'mal', 'management', 'may', 'me', 'mean', 'migration', 'mubarik', 'must', 'my', 'name', 'national', 'nbp', 'need', 'new', 'noc', 'objection', 'obtain', 'obtaining', 'of', 'offered', 'online', 'open', 'operate', 'option', 'or', 'oric', 'other', 'over', 'overseas', 'pakistan', 'pakistani', 'peef', 'please', 'policy', 'possible', 'private', 'procedure', 'professional', 'professor', 'program', 'prospectus', 'protection', 'province', 'punjab', 'qec', 'quality', 'question', 'raise', 'recent', 'recognized', 'recongnized', 'register', 'registered', 'registration', 'residing', 'review', 'rti', 's', 'schedule', 'scheme', 'scholarship', 'science', 'see', 'select', 'selection', 'selection/to', 'semester', 'service', 'short', 'should', 'sir', 'society', 'specialization', 'standard', 'startup', 'status', 'stipend', 'structure', 'student', 'study', 'study/degree', 'suitability', 'suitable', 'superintendent', 'support', 'syeda', 'syllabus', 'teacher', 'technology', 'tender', 'thank', 'thanks', 'that', 'the', 'there', 'through', 'to', 'transcript', 'transfer', 'ul', 'unfreeze', 'uniform', 'university', 'various', 'vendor', 'verification', 'verify', 'version', 'very', 'view', 'virtual', 'vspace', 'vu', 'vup', 'want', 'website', 'welfare', 'what', 'whats', 'where', 'who', 'whom', 'will', 'window', 'with', 'within', 'woman', 'worker', 'workplace', 'year', 'you', 'your', 'zero']\n"
          ]
        }
      ],
      "source": [
        "# lemmaztize and lower each word and remove duplicates\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")\n",
        "# classes = intents\n",
        "print (len(classes), \"classes\", classes)\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique lemmatized words\", words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(words,open('texts.pkl','wb'))\n",
        "pickle.dump(classes,open('labels.pkl','wb'))"
      ],
      "metadata": {
        "id": "xVeZGL_3N2As"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create our training data**"
      ],
      "metadata": {
        "id": "2NjkUQ6SOJ6q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZDzp9HIMs30",
        "outputId": "db4f67e6-cc06-49a4-b627-07d80a253055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data created\n"
          ]
        }
      ],
      "source": [
        "# create our training data\n",
        "training = []\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # lemmatize each word - create base word, in attempt to represent related words\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array with 1, if word match found in current pattern\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "\n",
        "# Padding to ensure consistent shape for bag-of-words representations\n",
        "max_len = max(len(sample[0]) for sample in training)  # Find the maximum length\n",
        "padded_training = []\n",
        "for sample in training:\n",
        "    bag, output_row = sample\n",
        "    # Check if padding is actually necessary\n",
        "    if len(bag) < max_len:\n",
        "        padded_bag = bag + [0] * (max_len - len(bag))  # Pad with zeros if needed\n",
        "    else:\n",
        "        padded_bag = bag  # No padding needed\n",
        "    padded_training.append([padded_bag, output_row])\n",
        "\n",
        "training = np.array(padded_training, dtype=object) # Convert the padded list to NumPy array, allow object dtype\n",
        "\n",
        "\n",
        "# create train and test lists. X - patterns, Y - intents\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Training data created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiA5mfO_bf5G"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4-Model Training**</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-gDV4j8l0L0"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.1-Neural Netowrk**</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em4aCw6ImU3R"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.1.1 Model Training**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GwFda9VsmgQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5a38de-a375-4cd6-8d02-134bc0e2d427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.0084 - loss: 4.5464\n",
            "Epoch 2/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0177 - loss: 4.4837\n",
            "Epoch 3/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0367 - loss: 4.4349\n",
            "Epoch 4/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0459 - loss: 4.4188\n",
            "Epoch 5/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0654 - loss: 4.3235\n",
            "Epoch 6/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0846 - loss: 4.1590\n",
            "Epoch 7/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0768 - loss: 4.1033\n",
            "Epoch 8/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1181 - loss: 3.9969\n",
            "Epoch 9/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1465 - loss: 3.8869\n",
            "Epoch 10/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1744 - loss: 3.7191\n",
            "Epoch 11/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1778 - loss: 3.4454\n",
            "Epoch 12/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2192 - loss: 3.3734\n",
            "Epoch 13/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2527 - loss: 3.2866\n",
            "Epoch 14/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2148 - loss: 3.1213\n",
            "Epoch 15/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2876 - loss: 3.1103\n",
            "Epoch 16/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3191 - loss: 2.8431\n",
            "Epoch 17/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3190 - loss: 2.7241\n",
            "Epoch 18/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3798 - loss: 2.4675\n",
            "Epoch 19/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3878 - loss: 2.5750\n",
            "Epoch 20/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3816 - loss: 2.3000\n",
            "Epoch 21/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4407 - loss: 2.2127\n",
            "Epoch 22/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4757 - loss: 2.1331\n",
            "Epoch 23/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4706 - loss: 2.0859\n",
            "Epoch 24/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4280 - loss: 2.0712\n",
            "Epoch 25/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4385 - loss: 1.8237\n",
            "Epoch 26/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5498 - loss: 1.6663\n",
            "Epoch 27/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5367 - loss: 1.7643\n",
            "Epoch 28/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4426 - loss: 1.9117\n",
            "Epoch 29/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4861 - loss: 1.7259\n",
            "Epoch 30/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5556 - loss: 1.5490\n",
            "Epoch 31/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6001 - loss: 1.4723\n",
            "Epoch 32/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5843 - loss: 1.4164\n",
            "Epoch 33/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6531 - loss: 1.1870\n",
            "Epoch 34/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6123 - loss: 1.2883\n",
            "Epoch 35/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6511 - loss: 1.1952\n",
            "Epoch 36/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6246 - loss: 1.1488\n",
            "Epoch 37/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7234 - loss: 1.0241\n",
            "Epoch 38/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6590 - loss: 1.1584\n",
            "Epoch 39/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6530 - loss: 1.1117\n",
            "Epoch 40/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6093 - loss: 1.4147\n",
            "Epoch 41/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7005 - loss: 1.1867\n",
            "Epoch 42/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6970 - loss: 1.0100\n",
            "Epoch 43/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6867 - loss: 1.0412\n",
            "Epoch 44/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6847 - loss: 1.0586\n",
            "Epoch 45/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7770 - loss: 0.7811\n",
            "Epoch 46/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7101 - loss: 0.9721\n",
            "Epoch 47/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7002 - loss: 1.0569\n",
            "Epoch 48/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6942 - loss: 0.9577\n",
            "Epoch 49/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7201 - loss: 0.9728\n",
            "Epoch 50/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7378 - loss: 0.8593\n",
            "Epoch 51/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7606 - loss: 0.7942\n",
            "Epoch 52/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7058 - loss: 0.9789\n",
            "Epoch 53/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7253 - loss: 0.7965\n",
            "Epoch 54/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7628 - loss: 0.7713\n",
            "Epoch 55/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7987 - loss: 0.7121\n",
            "Epoch 56/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7265 - loss: 0.8443\n",
            "Epoch 57/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7477 - loss: 0.9049\n",
            "Epoch 58/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8090 - loss: 0.6443\n",
            "Epoch 59/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7217 - loss: 0.9043\n",
            "Epoch 60/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7815 - loss: 0.7606\n",
            "Epoch 61/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6967 - loss: 0.9715\n",
            "Epoch 62/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6659 - loss: 1.0758\n",
            "Epoch 63/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7352 - loss: 0.7865\n",
            "Epoch 64/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7658 - loss: 0.6988\n",
            "Epoch 65/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7193 - loss: 0.8146\n",
            "Epoch 66/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7262 - loss: 0.9137\n",
            "Epoch 67/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7891 - loss: 0.7304\n",
            "Epoch 68/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8072 - loss: 0.7296\n",
            "Epoch 69/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7291 - loss: 0.8298\n",
            "Epoch 70/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7729 - loss: 0.7093\n",
            "Epoch 71/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8185 - loss: 0.6343\n",
            "Epoch 72/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7691 - loss: 0.7358\n",
            "Epoch 73/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7808 - loss: 0.7026\n",
            "Epoch 74/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7358 - loss: 0.7629\n",
            "Epoch 75/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7919 - loss: 0.6527\n",
            "Epoch 76/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8155 - loss: 0.6256\n",
            "Epoch 77/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8189 - loss: 0.6958\n",
            "Epoch 78/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8178 - loss: 0.5759\n",
            "Epoch 79/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7461 - loss: 0.6974\n",
            "Epoch 80/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8211 - loss: 0.5339\n",
            "Epoch 81/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7920 - loss: 0.7311\n",
            "Epoch 82/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7929 - loss: 0.7063\n",
            "Epoch 83/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8139 - loss: 0.6183\n",
            "Epoch 84/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8383 - loss: 0.6012\n",
            "Epoch 85/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8067 - loss: 0.5997\n",
            "Epoch 86/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7845 - loss: 0.7774\n",
            "Epoch 87/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8193 - loss: 0.5466\n",
            "Epoch 88/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8018 - loss: 0.5809\n",
            "Epoch 89/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8362 - loss: 0.5411\n",
            "Epoch 90/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8397 - loss: 0.5211\n",
            "Epoch 91/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8650 - loss: 0.5039\n",
            "Epoch 92/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8445 - loss: 0.5397\n",
            "Epoch 93/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8244 - loss: 0.5040\n",
            "Epoch 94/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8340 - loss: 0.5181\n",
            "Epoch 95/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8104 - loss: 0.8090\n",
            "Epoch 96/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8385 - loss: 0.4875\n",
            "Epoch 97/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8153 - loss: 0.5526\n",
            "Epoch 98/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8412 - loss: 0.4533\n",
            "Epoch 99/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8244 - loss: 0.5225\n",
            "Epoch 100/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8278 - loss: 0.5911\n",
            "Epoch 101/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8092 - loss: 0.5507\n",
            "Epoch 102/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8168 - loss: 0.5792\n",
            "Epoch 103/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7181 - loss: 0.8537\n",
            "Epoch 104/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8305 - loss: 0.5382\n",
            "Epoch 105/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8801 - loss: 0.5008\n",
            "Epoch 106/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8630 - loss: 0.4656\n",
            "Epoch 107/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8258 - loss: 0.5539\n",
            "Epoch 108/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8295 - loss: 0.4915\n",
            "Epoch 109/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8310 - loss: 0.4706\n",
            "Epoch 110/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8259 - loss: 0.6075\n",
            "Epoch 111/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8291 - loss: 0.5748\n",
            "Epoch 112/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8297 - loss: 0.5612\n",
            "Epoch 113/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8376 - loss: 0.5825\n",
            "Epoch 114/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8661 - loss: 0.4330\n",
            "Epoch 115/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7916 - loss: 0.6395\n",
            "Epoch 116/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8464 - loss: 0.4967\n",
            "Epoch 117/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8394 - loss: 0.4502\n",
            "Epoch 118/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8161 - loss: 0.5351\n",
            "Epoch 119/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7863 - loss: 0.7401\n",
            "Epoch 120/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8517 - loss: 0.5300\n",
            "Epoch 121/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8694 - loss: 0.5715\n",
            "Epoch 122/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7907 - loss: 0.6794\n",
            "Epoch 123/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8266 - loss: 0.5574\n",
            "Epoch 124/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8214 - loss: 0.5376\n",
            "Epoch 125/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8094 - loss: 0.7007\n",
            "Epoch 126/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8356 - loss: 0.5971\n",
            "Epoch 127/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8492 - loss: 0.4150\n",
            "Epoch 128/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8468 - loss: 0.4233\n",
            "Epoch 129/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7769 - loss: 0.6352\n",
            "Epoch 130/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8020 - loss: 0.6116\n",
            "Epoch 131/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8286 - loss: 0.5161\n",
            "Epoch 132/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8385 - loss: 0.4151\n",
            "Epoch 133/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8246 - loss: 0.4585\n",
            "Epoch 134/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8559 - loss: 0.5030\n",
            "Epoch 135/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8265 - loss: 0.6225\n",
            "Epoch 136/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8299 - loss: 0.4942\n",
            "Epoch 137/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8383 - loss: 0.4772\n",
            "Epoch 138/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8646 - loss: 0.4763\n",
            "Epoch 139/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7748 - loss: 0.7699\n",
            "Epoch 140/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8201 - loss: 0.5255\n",
            "Epoch 141/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8810 - loss: 0.3929\n",
            "Epoch 142/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8409 - loss: 0.4845\n",
            "Epoch 143/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8554 - loss: 0.4054\n",
            "Epoch 144/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8955 - loss: 0.3483\n",
            "Epoch 145/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8450 - loss: 0.4602\n",
            "Epoch 146/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8274 - loss: 0.6939\n",
            "Epoch 147/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8654 - loss: 0.4860\n",
            "Epoch 148/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7788 - loss: 0.7469\n",
            "Epoch 149/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7912 - loss: 0.6115\n",
            "Epoch 150/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8312 - loss: 0.5432\n",
            "Epoch 151/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8352 - loss: 0.6407\n",
            "Epoch 152/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8670 - loss: 0.4077\n",
            "Epoch 153/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8414 - loss: 0.5343\n",
            "Epoch 154/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8326 - loss: 0.4780\n",
            "Epoch 155/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8354 - loss: 0.4965\n",
            "Epoch 156/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8051 - loss: 0.5834\n",
            "Epoch 157/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7979 - loss: 0.6447\n",
            "Epoch 158/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8417 - loss: 0.5758\n",
            "Epoch 159/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8528 - loss: 0.4244\n",
            "Epoch 160/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8158 - loss: 0.4660\n",
            "Epoch 161/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7992 - loss: 0.6342\n",
            "Epoch 162/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8315 - loss: 0.5362\n",
            "Epoch 163/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8316 - loss: 0.6078\n",
            "Epoch 164/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8373 - loss: 0.5302\n",
            "Epoch 165/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8559 - loss: 0.4551\n",
            "Epoch 166/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8772 - loss: 0.4120\n",
            "Epoch 167/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8220 - loss: 0.5693\n",
            "Epoch 168/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8086 - loss: 0.7795\n",
            "Epoch 169/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8371 - loss: 0.6128\n",
            "Epoch 170/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8371 - loss: 0.4830\n",
            "Epoch 171/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8092 - loss: 0.5218\n",
            "Epoch 172/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7943 - loss: 0.5967\n",
            "Epoch 173/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8611 - loss: 0.5098\n",
            "Epoch 174/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8195 - loss: 0.6052\n",
            "Epoch 175/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8621 - loss: 0.5568\n",
            "Epoch 176/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8425 - loss: 0.4355\n",
            "Epoch 177/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8780 - loss: 0.4214\n",
            "Epoch 178/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8621 - loss: 0.4338\n",
            "Epoch 179/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8762 - loss: 0.3158\n",
            "Epoch 180/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8593 - loss: 0.3986\n",
            "Epoch 181/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8570 - loss: 0.5072\n",
            "Epoch 182/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7813 - loss: 0.8389\n",
            "Epoch 183/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8887 - loss: 0.4844\n",
            "Epoch 184/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8244 - loss: 0.5852\n",
            "Epoch 185/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8110 - loss: 0.5514\n",
            "Epoch 186/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8274 - loss: 0.6009\n",
            "Epoch 187/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8797 - loss: 0.4412\n",
            "Epoch 188/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7909 - loss: 0.6851\n",
            "Epoch 189/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8558 - loss: 0.5219\n",
            "Epoch 190/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8810 - loss: 0.4103\n",
            "Epoch 191/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8090 - loss: 0.4983\n",
            "Epoch 192/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8531 - loss: 0.4323\n",
            "Epoch 193/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8050 - loss: 0.6476\n",
            "Epoch 194/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8626 - loss: 0.4392\n",
            "Epoch 195/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8035 - loss: 0.5686\n",
            "Epoch 196/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8408 - loss: 0.4837\n",
            "Epoch 197/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8494 - loss: 0.5497\n",
            "Epoch 198/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8701 - loss: 0.4014\n",
            "Epoch 199/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8352 - loss: 0.4338\n",
            "Epoch 200/200\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8288 - loss: 0.5046\n",
            "model created\n"
          ]
        }
      ],
      "source": [
        "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
        "# equal to number of intents to predict output intent with softmax\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True) # Changed 'lr' to 'learning_rate'\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chat_model.keras', hist) # Changed 'chat_model' to 'chat_model.keras' to include the .keras extension\n",
        "\n",
        "print(\"model created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HvFngP6nzWA"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.2- LSTM**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1nVA9smHqt4v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Flatten,Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLxnK1_RLO4a"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.2.1 - Model Architecture 1**</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "kcFSuMKmq2C_",
        "outputId": "6ad4a9ee-ed7b-481d-9484-c5cc871314c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m290\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m16,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m290\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m290\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m290\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │          \u001b[38;5;34m12,416\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m290\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m290\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │           \u001b[38;5;34m3,136\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │              \u001b[38;5;34m64\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m)                  │           \u001b[38;5;34m1,581\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">290</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">290</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">290</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">290</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">290</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">290</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,581</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,477\u001b[0m (134.68 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,477</span> (134.68 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,253\u001b[0m (133.80 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,253</span> (133.80 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ExponentialDecay' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-4cc065bcffa9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m lr_schedule = ExponentialDecay(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdecay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ExponentialDecay' is not defined"
          ]
        }
      ],
      "source": [
        "#Create the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(len(train_x[0]),1), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(32,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(16,return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "model.summary()\n",
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "#adam = Adam(learning_rate=lr_schedule)\n",
        "sgd = SGD(learning_rate=0.01 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qQz12ssLy1C"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.2.2 - Model Architecture 2**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUkU6STQLw-S",
        "outputId": "784af326-42e3-48f5-fe08-64e505a446d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_7 (LSTM)               (None, 353, 128)          66560     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 353, 128)          0         \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 353, 64)           49408     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 353, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 353, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 353, 32)           12416     \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 353, 32)           0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 353, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (None, 16)                3136      \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 16)                64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 115)               1955      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 133923 (523.14 KB)\n",
            "Trainable params: 133699 (522.26 KB)\n",
            "Non-trainable params: 224 (896.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "817/817 [==============================] - 56s 60ms/step - loss: 4.6774 - accuracy: 0.0317\n",
            "Epoch 2/100\n",
            "817/817 [==============================] - 49s 60ms/step - loss: 4.3611 - accuracy: 0.0743\n",
            "Epoch 3/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 4.1001 - accuracy: 0.0929\n",
            "Epoch 4/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 3.8784 - accuracy: 0.1086\n",
            "Epoch 5/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 3.6722 - accuracy: 0.1272\n",
            "Epoch 6/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 3.4553 - accuracy: 0.1532\n",
            "Epoch 7/100\n",
            "817/817 [==============================] - 47s 58ms/step - loss: 3.2476 - accuracy: 0.1838\n",
            "Epoch 8/100\n",
            "817/817 [==============================] - 48s 58ms/step - loss: 3.0509 - accuracy: 0.2141\n",
            "Epoch 9/100\n",
            "817/817 [==============================] - 48s 58ms/step - loss: 2.8726 - accuracy: 0.2390\n",
            "Epoch 10/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 2.7202 - accuracy: 0.2650\n",
            "Epoch 11/100\n",
            "817/817 [==============================] - 48s 58ms/step - loss: 2.5775 - accuracy: 0.2919\n",
            "Epoch 12/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 2.4175 - accuracy: 0.3243\n",
            "Epoch 13/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 2.2523 - accuracy: 0.3631\n",
            "Epoch 14/100\n",
            "817/817 [==============================] - 48s 58ms/step - loss: 2.0835 - accuracy: 0.3977\n",
            "Epoch 15/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.9249 - accuracy: 0.4319\n",
            "Epoch 16/100\n",
            "817/817 [==============================] - 49s 59ms/step - loss: 1.7853 - accuracy: 0.4622\n",
            "Epoch 17/100\n",
            "817/817 [==============================] - 49s 60ms/step - loss: 1.6644 - accuracy: 0.4909\n",
            "Epoch 18/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.5532 - accuracy: 0.5150\n",
            "Epoch 19/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.4555 - accuracy: 0.5411\n",
            "Epoch 20/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.3741 - accuracy: 0.5610\n",
            "Epoch 21/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.3118 - accuracy: 0.5780\n",
            "Epoch 22/100\n",
            "817/817 [==============================] - 49s 59ms/step - loss: 1.2368 - accuracy: 0.5959\n",
            "Epoch 23/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.1680 - accuracy: 0.6165\n",
            "Epoch 24/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.1144 - accuracy: 0.6316\n",
            "Epoch 25/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.0594 - accuracy: 0.6488\n",
            "Epoch 26/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.0047 - accuracy: 0.6664\n",
            "Epoch 27/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 0.9628 - accuracy: 0.6805\n",
            "Epoch 28/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 0.9011 - accuracy: 0.6978\n",
            "Epoch 29/100\n",
            " 35/817 [>.............................] - ETA: 45s - loss: 0.9423 - accuracy: 0.6922"
          ]
        }
      ],
      "source": [
        "#Create the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(len(train_x[0]),1), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(32,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(16,return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "model.summary()\n",
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "#adam = Adam(learning_rate=lr_schedule)\n",
        "sgd = SGD(learning_rate=0.001 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGcC29CeWi4y"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.2.2.1 - Model Saving**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueVFk8ZWYS4s",
        "outputId": "1be94ae0-6820-4b9f-bcd0-a103aad8f244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model created and saved\n"
          ]
        }
      ],
      "source": [
        "model.save( path_to_save_model+'chat_model_2', hist)\n",
        "print(\"model created and saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezj7AyRaQ1Kl"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.2.3 - Model Architecture 3**</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cJqEH_qWb-8",
        "outputId": "21242e2e-0468-4289-b301-1f70a88fe9ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 391, 128)          33792     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 391, 128)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 391, 64)           41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 391, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 391, 64)           256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 32)                10368     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 113)               3729      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 89489 (349.57 KB)\n",
            "Trainable params: 89297 (348.82 KB)\n",
            "Non-trainable params: 192 (768.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(units=64,input_shape=(len(train_x[0]),1),return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(units=32,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=16,return_sequences=False)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "\n",
        "# Build the model\n",
        "model.build(input_shape=(None, len(train_x[0]), 1))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57ZFEBvntTGt",
        "outputId": "f96deecc-a620-4aa9-f7a7-a27bada9adf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "932/932 [==============================] - 89s 80ms/step - loss: 4.5237 - accuracy: 0.0520\n",
            "Epoch 2/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 3.7492 - accuracy: 0.1474\n",
            "Epoch 3/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 3.2352 - accuracy: 0.2316\n",
            "Epoch 4/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 2.7851 - accuracy: 0.3036\n",
            "Epoch 5/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 2.4090 - accuracy: 0.3725\n",
            "Epoch 6/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 2.0772 - accuracy: 0.4478\n",
            "Epoch 7/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 1.7939 - accuracy: 0.5148\n",
            "Epoch 8/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 1.5713 - accuracy: 0.5678\n",
            "Epoch 9/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 1.3666 - accuracy: 0.6160\n",
            "Epoch 10/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 1.2013 - accuracy: 0.6577\n",
            "Epoch 11/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 1.1264 - accuracy: 0.6824\n",
            "Epoch 12/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 1.2004 - accuracy: 0.6516\n",
            "Epoch 13/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.8443 - accuracy: 0.7513\n",
            "Epoch 14/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.7252 - accuracy: 0.7837\n",
            "Epoch 15/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.8774 - accuracy: 0.7444\n",
            "Epoch 16/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.6226 - accuracy: 0.8124\n",
            "Epoch 17/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.5666 - accuracy: 0.8262\n",
            "Epoch 18/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.5038 - accuracy: 0.8444\n",
            "Epoch 19/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 0.4577 - accuracy: 0.8591\n",
            "Epoch 20/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 0.4877 - accuracy: 0.8506\n",
            "Epoch 21/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.3760 - accuracy: 0.8852\n",
            "Epoch 22/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 0.7034 - accuracy: 0.8027\n",
            "Epoch 23/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.4057 - accuracy: 0.8716\n",
            "Epoch 24/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.3222 - accuracy: 0.9016\n",
            "Epoch 25/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.2788 - accuracy: 0.9162\n",
            "Epoch 26/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.2577 - accuracy: 0.9236\n",
            "Epoch 27/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.2141 - accuracy: 0.9397\n",
            "Epoch 28/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1942 - accuracy: 0.9459\n",
            "Epoch 29/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1748 - accuracy: 0.9508\n",
            "Epoch 30/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1582 - accuracy: 0.9571\n",
            "Epoch 31/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.1455 - accuracy: 0.9603\n",
            "Epoch 32/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.1420 - accuracy: 0.9614\n",
            "Epoch 33/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1262 - accuracy: 0.9656\n",
            "Epoch 34/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.1160 - accuracy: 0.9687\n",
            "Epoch 35/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.1170 - accuracy: 0.9677\n",
            "Epoch 36/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1075 - accuracy: 0.9699\n",
            "Epoch 37/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0997 - accuracy: 0.9726\n",
            "Epoch 38/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0975 - accuracy: 0.9725\n",
            "Epoch 39/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0927 - accuracy: 0.9743\n",
            "Epoch 40/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0884 - accuracy: 0.9760\n",
            "Epoch 41/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1597 - accuracy: 0.9528\n",
            "Epoch 42/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.0862 - accuracy: 0.9760\n",
            "Epoch 43/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0803 - accuracy: 0.9778\n",
            "Epoch 44/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0789 - accuracy: 0.9787\n",
            "Epoch 45/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0696 - accuracy: 0.9814\n",
            "Epoch 46/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.1011 - accuracy: 0.9714\n",
            "Epoch 47/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0888 - accuracy: 0.9745\n",
            "Epoch 48/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.0687 - accuracy: 0.9809\n",
            "Epoch 49/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0600 - accuracy: 0.9840\n",
            "Epoch 50/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0578 - accuracy: 0.9844\n",
            "Epoch 51/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0562 - accuracy: 0.9844\n",
            "Epoch 52/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0529 - accuracy: 0.9862\n",
            "Epoch 53/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0539 - accuracy: 0.9853\n",
            "Epoch 54/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0554 - accuracy: 0.9852\n",
            "Epoch 55/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0486 - accuracy: 0.9873\n",
            "Epoch 56/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0555 - accuracy: 0.9848\n",
            "Epoch 57/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.0472 - accuracy: 0.9875\n",
            "Epoch 58/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0434 - accuracy: 0.9885\n",
            "Epoch 59/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0437 - accuracy: 0.9883\n",
            "Epoch 60/100\n",
            "932/932 [==============================] - 77s 83ms/step - loss: 0.0482 - accuracy: 0.9866\n",
            "Epoch 61/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 1.8266 - accuracy: 0.6585\n",
            "Epoch 62/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.6956 - accuracy: 0.7751\n",
            "Epoch 63/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.3384 - accuracy: 0.8836\n",
            "Epoch 64/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.2430 - accuracy: 0.9169\n",
            "Epoch 65/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.1965 - accuracy: 0.9355\n",
            "Epoch 66/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1629 - accuracy: 0.9475\n",
            "Epoch 67/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1416 - accuracy: 0.9549\n",
            "Epoch 68/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1350 - accuracy: 0.9579\n",
            "Epoch 69/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.1189 - accuracy: 0.9626\n",
            "Epoch 70/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.1042 - accuracy: 0.9680\n",
            "Epoch 71/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.9430 - accuracy: 0.7589\n",
            "Epoch 72/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.2478 - accuracy: 0.9144\n",
            "Epoch 73/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1939 - accuracy: 0.9343\n",
            "Epoch 74/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.1517 - accuracy: 0.9503\n",
            "Epoch 75/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1292 - accuracy: 0.9583\n",
            "Epoch 76/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1177 - accuracy: 0.9625\n",
            "Epoch 77/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1046 - accuracy: 0.9674\n",
            "Epoch 78/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1538 - accuracy: 0.9545\n",
            "Epoch 79/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0958 - accuracy: 0.9706\n",
            "Epoch 80/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0812 - accuracy: 0.9749\n",
            "Epoch 81/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0730 - accuracy: 0.9781\n",
            "Epoch 82/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0717 - accuracy: 0.9784\n",
            "Epoch 83/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0665 - accuracy: 0.9798\n",
            "Epoch 84/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0638 - accuracy: 0.9807\n",
            "Epoch 85/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0615 - accuracy: 0.9810\n",
            "Epoch 86/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0605 - accuracy: 0.9812\n",
            "Epoch 87/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0541 - accuracy: 0.9832\n",
            "Epoch 88/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0606 - accuracy: 0.9812\n",
            "Epoch 89/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0531 - accuracy: 0.9838\n",
            "Epoch 90/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0498 - accuracy: 0.9844\n",
            "Epoch 91/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0480 - accuracy: 0.9844\n",
            "Epoch 92/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0460 - accuracy: 0.9856\n",
            "Epoch 93/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0445 - accuracy: 0.9858\n",
            "Epoch 94/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0455 - accuracy: 0.9854\n",
            "Epoch 95/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0434 - accuracy: 0.9862\n",
            "Epoch 96/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0416 - accuracy: 0.9865\n",
            "Epoch 97/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0432 - accuracy: 0.9860\n",
            "Epoch 98/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.0406 - accuracy: 0.9869\n",
            "Epoch 99/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0404 - accuracy: 0.9867\n",
            "Epoch 100/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0397 - accuracy: 0.9872\n"
          ]
        }
      ],
      "source": [
        "sgd = SGD(learning_rate=0.001 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiXTxc8XNsYE",
        "outputId": "6974d4b2-d88e-41c2-d2d1-d55be208fc22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model created and saved\n"
          ]
        }
      ],
      "source": [
        "model.save( path_to_save_model+'chat_model_3', hist)\n",
        "print(\"model created and saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQWlRcZXEmuq"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.2.4 - Model Architecture 4**</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIS8x11cEzeT",
        "outputId": "d088e33b-d89c-4935-95a9-c3f42273b025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 391, 256)          133120    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 391, 256)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 391, 128)          164352    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 391, 128)          0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 391, 128)          512       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 391, 64)           41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 391, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 391, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 32)                10368     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 113)               3729      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 353681 (1.35 MB)\n",
            "Trainable params: 353233 (1.35 MB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "932/932 [==============================] - 141s 132ms/step - loss: 4.6924 - accuracy: 0.0300\n",
            "Epoch 2/100\n",
            "932/932 [==============================] - 125s 134ms/step - loss: 4.0253 - accuracy: 0.1153\n",
            "Epoch 3/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 3.4509 - accuracy: 0.2022\n",
            "Epoch 4/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 2.9579 - accuracy: 0.2890\n",
            "Epoch 5/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 2.5412 - accuracy: 0.3532\n",
            "Epoch 6/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 2.1979 - accuracy: 0.4147\n",
            "Epoch 7/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 1.8938 - accuracy: 0.4812\n",
            "Epoch 8/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 1.6091 - accuracy: 0.5525\n",
            "Epoch 9/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 1.3446 - accuracy: 0.6254\n",
            "Epoch 10/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 1.1106 - accuracy: 0.6866\n",
            "Epoch 11/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.9190 - accuracy: 0.7362\n",
            "Epoch 12/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.7668 - accuracy: 0.7779\n",
            "Epoch 13/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.6491 - accuracy: 0.8119\n",
            "Epoch 14/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.5574 - accuracy: 0.8390\n",
            "Epoch 15/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.4864 - accuracy: 0.8608\n",
            "Epoch 16/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.4312 - accuracy: 0.8749\n",
            "Epoch 17/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.3855 - accuracy: 0.8877\n",
            "Epoch 18/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.3482 - accuracy: 0.8984\n",
            "Epoch 19/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.3189 - accuracy: 0.9063\n",
            "Epoch 20/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2918 - accuracy: 0.9139\n",
            "Epoch 21/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2675 - accuracy: 0.9213\n",
            "Epoch 22/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2470 - accuracy: 0.9265\n",
            "Epoch 23/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2308 - accuracy: 0.9308\n",
            "Epoch 24/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2167 - accuracy: 0.9358\n",
            "Epoch 25/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2019 - accuracy: 0.9393\n",
            "Epoch 26/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1881 - accuracy: 0.9448\n",
            "Epoch 27/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1775 - accuracy: 0.9467\n",
            "Epoch 28/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1664 - accuracy: 0.9496\n",
            "Epoch 29/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1594 - accuracy: 0.9519\n",
            "Epoch 30/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1498 - accuracy: 0.9537\n",
            "Epoch 31/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1445 - accuracy: 0.9548\n",
            "Epoch 32/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1361 - accuracy: 0.9577\n",
            "Epoch 33/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1311 - accuracy: 0.9586\n",
            "Epoch 34/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1246 - accuracy: 0.9605\n",
            "Epoch 35/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1200 - accuracy: 0.9617\n",
            "Epoch 36/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1172 - accuracy: 0.9624\n",
            "Epoch 37/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1096 - accuracy: 0.9653\n",
            "Epoch 38/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1062 - accuracy: 0.9660\n",
            "Epoch 39/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1007 - accuracy: 0.9686\n",
            "Epoch 40/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0979 - accuracy: 0.9699\n",
            "Epoch 41/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0938 - accuracy: 0.9707\n",
            "Epoch 42/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0871 - accuracy: 0.9733\n",
            "Epoch 43/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0822 - accuracy: 0.9747\n",
            "Epoch 44/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.0799 - accuracy: 0.9753\n",
            "Epoch 45/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0785 - accuracy: 0.9761\n",
            "Epoch 46/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0783 - accuracy: 0.9757\n",
            "Epoch 47/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0706 - accuracy: 0.9781\n",
            "Epoch 48/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0688 - accuracy: 0.9789\n",
            "Epoch 49/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0691 - accuracy: 0.9782\n",
            "Epoch 50/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.0676 - accuracy: 0.9787\n",
            "Epoch 51/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0601 - accuracy: 0.9814\n",
            "Epoch 52/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0681 - accuracy: 0.9784\n",
            "Epoch 53/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0582 - accuracy: 0.9815\n",
            "Epoch 54/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0592 - accuracy: 0.9810\n",
            "Epoch 55/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.0552 - accuracy: 0.9819\n",
            "Epoch 56/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0534 - accuracy: 0.9820\n",
            "Epoch 57/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0519 - accuracy: 0.9827\n",
            "Epoch 58/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0506 - accuracy: 0.9833\n",
            "Epoch 59/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.0542 - accuracy: 0.9821\n",
            "Epoch 60/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0476 - accuracy: 0.9842\n",
            "Epoch 61/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0461 - accuracy: 0.9852\n",
            "Epoch 62/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0472 - accuracy: 0.9846\n",
            "Epoch 63/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0525 - accuracy: 0.9833\n",
            "Epoch 64/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0510 - accuracy: 0.9833\n",
            "Epoch 65/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0464 - accuracy: 0.9848\n",
            "Epoch 66/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0439 - accuracy: 0.9861\n",
            "Epoch 67/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0438 - accuracy: 0.9864\n",
            "Epoch 68/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0454 - accuracy: 0.9846\n",
            "Epoch 69/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0428 - accuracy: 0.9854\n",
            "Epoch 70/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0460 - accuracy: 0.9835\n",
            "Epoch 71/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0442 - accuracy: 0.9853\n",
            "Epoch 72/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0410 - accuracy: 0.9851\n",
            "Epoch 73/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0407 - accuracy: 0.9858\n",
            "Epoch 74/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0404 - accuracy: 0.9863\n",
            "Epoch 75/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0368 - accuracy: 0.9878\n",
            "Epoch 76/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0367 - accuracy: 0.9879\n",
            "Epoch 77/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0348 - accuracy: 0.9885\n",
            "Epoch 78/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0339 - accuracy: 0.9886\n",
            "Epoch 79/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0333 - accuracy: 0.9891\n",
            "Epoch 80/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0364 - accuracy: 0.9874\n",
            "Epoch 81/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0327 - accuracy: 0.9890\n",
            "Epoch 82/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0325 - accuracy: 0.9893\n",
            "Epoch 83/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0318 - accuracy: 0.9897\n",
            "Epoch 84/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0301 - accuracy: 0.9903\n",
            "Epoch 85/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0294 - accuracy: 0.9904\n",
            "Epoch 86/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0298 - accuracy: 0.9904\n",
            "Epoch 87/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0314 - accuracy: 0.9900\n",
            "Epoch 88/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0291 - accuracy: 0.9906\n",
            "Epoch 89/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0418 - accuracy: 0.9866\n",
            "Epoch 90/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0301 - accuracy: 0.9901\n",
            "Epoch 91/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0266 - accuracy: 0.9914\n",
            "Epoch 92/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0265 - accuracy: 0.9918\n",
            "Epoch 93/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0262 - accuracy: 0.9920\n",
            "Epoch 94/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0262 - accuracy: 0.9917\n",
            "Epoch 95/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0258 - accuracy: 0.9917\n",
            "Epoch 96/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0256 - accuracy: 0.9916\n",
            "Epoch 97/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0292 - accuracy: 0.9907\n",
            "Epoch 98/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0275 - accuracy: 0.9914\n",
            "Epoch 99/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0256 - accuracy: 0.9918\n",
            "Epoch 100/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0231 - accuracy: 0.9927\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(units=128,input_shape=(len(train_x[0]),1),return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(units=64,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=32,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=16,return_sequences=False)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "\n",
        "# Build the model\n",
        "model.build(input_shape=(None, len(train_x[0]), 1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "sgd = SGD(learning_rate=0.001 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pn5pr4HjEmJ",
        "outputId": "38ff6c94-d6ef-4a1a-a890-3130ff589e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model created and saved\n"
          ]
        }
      ],
      "source": [
        "model.save( path_to_save_model+'chat_model_4', hist)\n",
        "print(\"model created and saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X71mU4ZljETL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeHAlQgIjDpx",
        "outputId": "de9b52ff-3d23-4d8d-d62c-1e55b5094734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 391, 256)          133120    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 391, 256)          1024      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 391, 128)          164352    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 391, 128)          0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 391, 64)           41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 391, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 391, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 32)                10368     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 113)               3729      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 354193 (1.35 MB)\n",
            "Trainable params: 353489 (1.35 MB)\n",
            "Non-trainable params: 704 (2.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "932/932 [==============================] - 145s 137ms/step - loss: 4.6558 - accuracy: 0.0336\n",
            "Epoch 2/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 4.1301 - accuracy: 0.1049\n",
            "Epoch 3/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 3.7473 - accuracy: 0.1478\n",
            "Epoch 4/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 3.4378 - accuracy: 0.1868\n",
            "Epoch 5/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 3.1494 - accuracy: 0.2339\n",
            "Epoch 6/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 2.8716 - accuracy: 0.2799\n",
            "Epoch 7/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 2.6211 - accuracy: 0.3216\n",
            "Epoch 8/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 2.4007 - accuracy: 0.3596\n",
            "Epoch 9/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 2.2027 - accuracy: 0.3998\n",
            "Epoch 10/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 2.0192 - accuracy: 0.4404\n",
            "Epoch 11/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 1.8442 - accuracy: 0.4803\n",
            "Epoch 12/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.6993 - accuracy: 0.5156\n",
            "Epoch 13/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.5580 - accuracy: 0.5511\n",
            "Epoch 14/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.4138 - accuracy: 0.5903\n",
            "Epoch 15/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 1.2874 - accuracy: 0.6236\n",
            "Epoch 16/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.1761 - accuracy: 0.6543\n",
            "Epoch 17/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.0703 - accuracy: 0.6818\n",
            "Epoch 18/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.9511 - accuracy: 0.7191\n",
            "Epoch 19/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.9754 - accuracy: 0.7076\n",
            "Epoch 20/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.8029 - accuracy: 0.7618\n",
            "Epoch 21/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.7363 - accuracy: 0.7807\n",
            "Epoch 22/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.6862 - accuracy: 0.7938\n",
            "Epoch 23/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.6902 - accuracy: 0.7925\n",
            "Epoch 24/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.5948 - accuracy: 0.8207\n",
            "Epoch 25/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.5565 - accuracy: 0.8309\n",
            "Epoch 26/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.5487 - accuracy: 0.8341\n",
            "Epoch 27/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.5008 - accuracy: 0.8481\n",
            "Epoch 28/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.4716 - accuracy: 0.8583\n",
            "Epoch 29/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.4422 - accuracy: 0.8665\n",
            "Epoch 30/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.4298 - accuracy: 0.8706\n",
            "Epoch 31/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.6385 - accuracy: 0.8112\n",
            "Epoch 32/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.5284 - accuracy: 0.8354\n",
            "Epoch 33/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.3811 - accuracy: 0.8849\n",
            "Epoch 34/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.3357 - accuracy: 0.8987\n",
            "Epoch 35/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.3031 - accuracy: 0.9090\n",
            "Epoch 36/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.4038 - accuracy: 0.8822\n",
            "Epoch 37/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2943 - accuracy: 0.9132\n",
            "Epoch 38/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2569 - accuracy: 0.9228\n",
            "Epoch 39/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.5749 - accuracy: 0.8398\n",
            "Epoch 40/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2453 - accuracy: 0.9276\n",
            "Epoch 41/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2448 - accuracy: 0.9261\n",
            "Epoch 42/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2230 - accuracy: 0.9331\n",
            "Epoch 43/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2210 - accuracy: 0.9344\n",
            "Epoch 44/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2366 - accuracy: 0.9296\n",
            "Epoch 45/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2673 - accuracy: 0.9200\n",
            "Epoch 46/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1893 - accuracy: 0.9443\n",
            "Epoch 47/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2203 - accuracy: 0.9347\n",
            "Epoch 48/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1895 - accuracy: 0.9435\n",
            "Epoch 49/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1884 - accuracy: 0.9439\n",
            "Epoch 50/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1903 - accuracy: 0.9426\n",
            "Epoch 51/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.1809 - accuracy: 0.9463\n",
            "Epoch 52/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.1735 - accuracy: 0.9479\n",
            "Epoch 53/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1785 - accuracy: 0.9456\n",
            "Epoch 54/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.1548 - accuracy: 0.7643\n",
            "Epoch 55/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.3068 - accuracy: 0.6001\n",
            "Epoch 56/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.3468 - accuracy: 0.6587\n",
            "Epoch 57/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 2.1607 - accuracy: 0.4120\n",
            "Epoch 58/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 1.1217 - accuracy: 0.6412\n",
            "Epoch 59/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.7630 - accuracy: 0.7456\n",
            "Epoch 60/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.5916 - accuracy: 0.8025\n",
            "Epoch 61/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.5010 - accuracy: 0.8335\n",
            "Epoch 62/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.4271 - accuracy: 0.8602\n",
            "Epoch 63/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.3834 - accuracy: 0.8765\n",
            "Epoch 64/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.3504 - accuracy: 0.8858\n",
            "Epoch 65/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.3145 - accuracy: 0.8993\n",
            "Epoch 66/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2871 - accuracy: 0.9081\n",
            "Epoch 67/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.2809 - accuracy: 0.9097\n",
            "Epoch 68/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2499 - accuracy: 0.9205\n",
            "Epoch 69/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2296 - accuracy: 0.9271\n",
            "Epoch 70/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.2267 - accuracy: 0.9280\n",
            "Epoch 71/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2176 - accuracy: 0.9305\n",
            "Epoch 72/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.2333 - accuracy: 0.9261\n",
            "Epoch 73/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2141 - accuracy: 0.9325\n",
            "Epoch 74/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2009 - accuracy: 0.9362\n",
            "Epoch 75/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1968 - accuracy: 0.9375\n",
            "Epoch 76/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2325 - accuracy: 0.9285\n",
            "Epoch 77/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1756 - accuracy: 0.9442\n",
            "Epoch 78/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1708 - accuracy: 0.9459\n",
            "Epoch 79/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1660 - accuracy: 0.9473\n",
            "Epoch 80/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1923 - accuracy: 0.9391\n",
            "Epoch 81/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.1514 - accuracy: 0.9520\n",
            "Epoch 82/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1617 - accuracy: 0.9489\n",
            "Epoch 83/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1406 - accuracy: 0.9558\n",
            "Epoch 84/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1413 - accuracy: 0.9550\n",
            "Epoch 85/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1374 - accuracy: 0.9566\n",
            "Epoch 86/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1682 - accuracy: 0.9468\n",
            "Epoch 87/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1444 - accuracy: 0.9540\n",
            "Epoch 88/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1271 - accuracy: 0.9593\n",
            "Epoch 89/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1262 - accuracy: 0.9602\n",
            "Epoch 90/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1205 - accuracy: 0.9622\n",
            "Epoch 91/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1179 - accuracy: 0.9625\n",
            "Epoch 92/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 0.1276 - accuracy: 0.9594\n",
            "Epoch 93/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 0.1176 - accuracy: 0.9630\n",
            "Epoch 94/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 0.1193 - accuracy: 0.9627\n",
            "Epoch 95/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 0.1077 - accuracy: 0.9659\n",
            "Epoch 96/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1090 - accuracy: 0.9656\n",
            "Epoch 97/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1111 - accuracy: 0.9654\n",
            "Epoch 98/100\n",
            "409/932 [============>.................] - ETA: 1:11 - loss: 0.1219 - accuracy: 0.9610"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(units=128,input_shape=(len(train_x[0]),1),return_sequences=True)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=64,return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(units=32,return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=16,return_sequences=False)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "\n",
        "# Build the model\n",
        "model.build(input_shape=(None, len(train_x[0]), 1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "sgd = SGD(learning_rate=0.001 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVKnyKo1Ryg3"
      },
      "outputs": [],
      "source": [
        "model.save( path_to_save_model+'chat_model_5', hist)\n",
        "print(\"model created and saved\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}